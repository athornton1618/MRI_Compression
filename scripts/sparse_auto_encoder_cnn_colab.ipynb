{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastmri\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFDFxEyHKpLy",
        "outputId": "16ea2471-2874-4186-b40f-d57d1c336b57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastmri\n",
            "  Downloading fastmri-0.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 82 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from fastmri) (0.12.0+cu113)\n",
            "Collecting pytorch-lightning<1.1,>=1.0.6\n",
            "  Downloading pytorch_lightning-1.0.8-py3-none-any.whl (561 kB)\n",
            "\u001b[K     |████████████████████████████████| 561 kB 136 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.16.2 in /usr/local/lib/python3.7/dist-packages (from fastmri) (0.18.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from fastmri) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastmri) (1.11.0+cu113)\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 351 kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from fastmri) (3.1.0)\n",
            "Collecting runstats>=1.8.0\n",
            "  Downloading runstats-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (350 kB)\n",
            "\u001b[K     |████████████████████████████████| 350 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->fastmri) (1.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<1.1,>=1.0.6->fastmri) (2.8.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning<1.1,>=1.0.6->fastmri) (4.64.0)\n",
            "Collecting fsspec>=0.8.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 46.7 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.2->fastmri) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.2->fastmri) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.2->fastmri) (2021.11.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.2->fastmri) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.2->fastmri) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.2->fastmri) (2.4.1)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.2->fastmri) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.2->fastmri) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.2->fastmri) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.2->fastmri) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.2->fastmri) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.2->fastmri) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.2->fastmri) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (1.44.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning<1.1,>=1.0.6->fastmri) (3.2.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=287b2b0cdbac62fb1daf04e573526c012c27b88fe00c4b561bf064b1648c0c04\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: PyYAML, future, fsspec, runstats, pytorch-lightning, fastmri\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 fastmri-0.1.1 fsspec-2022.3.0 future-0.18.2 pytorch-lightning-1.0.8 runstats-2.0.0\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.5-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do all of common functions here\n",
        "import numpy as np\n",
        "import pywt\n",
        "import bokeh\n",
        "import bokeh.plotting as bpl\n",
        "from bokeh.models import ColorBar, BasicTicker, LinearColorMapper\n",
        "import matplotlib\n",
        "import fastmri\n",
        "from fastmri.data import transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "# Some functions taken from HW1 - MRI\n",
        "\n",
        "## Standard basis element function for matrix space (ZERO-INDEXED)\n",
        "def stdbel(n, i, j):\n",
        "  E = np.zeros((n, n))\n",
        "  E[i, j] = 1\n",
        "  return E\n",
        "\n",
        "## Try to do something like imagesc in MATLAB using Bokeh tools.\n",
        "def imagesc(M, title=''):\n",
        "  m, n = M.shape\n",
        "  \n",
        "  # 600 px should be good; calculate ph to try to get aspect ratio right\n",
        "  pw = 600\n",
        "  ph = round(1.0 * pw * m / n)\n",
        "  h = bpl.figure(plot_width = pw, plot_height = ph, x_range=(0, 1.0*n),\n",
        "                 y_range=(0, 1.0*m), toolbar_location='below',\n",
        "                 title=title, match_aspect=True\n",
        "                )\n",
        "  \n",
        "  minval = np.min(M)\n",
        "  maxval = np.max(M)\n",
        "  \n",
        "  color_mapper = LinearColorMapper(palette=\"Greys256\", low=minval, high=maxval)\n",
        "  h.image(image=[M], x=0, y=0, dw=1.0*n, dh=1.0*m, color_mapper=color_mapper)\n",
        "  \n",
        "  color_bar = ColorBar(color_mapper=color_mapper, ticker=BasicTicker(),\n",
        "                      label_standoff=12, border_line_color=None, location=(0, 0))\n",
        "  \n",
        "  h.add_layout(color_bar, 'right')\n",
        "  \n",
        "\n",
        "  bpl.show(h)\n",
        "  return h\n",
        "\n",
        "# Get a default slice object for a multilevel wavelet transform\n",
        "# Used to abstract this annoying notation out of the transform...\n",
        "def default_slices(levels, n):\n",
        "  c = pywt.wavedec2(np.zeros((n, n)), 'db4', mode='periodization', level=levels)\n",
        "  bye, slices = pywt.coeffs_to_array(c)\n",
        "  return slices\n",
        "\n",
        "# Wrapper for forward discrete wavelet transform\n",
        "# Output data as a matrix (we don't care about tuple format)\n",
        "def dwt(levels, sdom_data):\n",
        "  c = pywt.wavedec2(sdom_data, 'db4', mode='periodization', level=levels)\n",
        "  output, bye = pywt.coeffs_to_array(c)\n",
        "  return output\n",
        "\n",
        "# Wrapper for inverse discrete wavelet transform\n",
        "# Expect wdom_data as a matrix (we don't care about tuple format)\n",
        "def idwt(levels, wdom_data, slices=None):\n",
        "  n = wdom_data.shape[0]\n",
        "  if slices is None:\n",
        "    slices = default_slices(levels, n)\n",
        "  c = pywt.array_to_coeffs(wdom_data, slices, output_format='wavedec2')\n",
        "  return pywt.waverec2(c, 'db4', mode='periodization')\n",
        "\n",
        "def frob_error(X_reconstruct,X):\n",
        "  num = np.linalg.norm(X_reconstruct-X, ord='fro')\n",
        "  den = np.linalg.norm(X, ord='fro')\n",
        "  return num/den\n",
        "\n",
        "def k_encode(k,X):\n",
        "  X_dwt = dwt(levels=3,sdom_data=X)\n",
        "  X_sorted = np.sort(np.absolute(np.array(X_dwt)).flatten(),kind='quicksort')[::-1]\n",
        "  k_threshold = X_sorted[k-1]\n",
        "  X_dwt[np.absolute(X_dwt)<k_threshold] = 0\n",
        "  X_encode = X_dwt\n",
        "  return X_encode\n",
        "\n",
        "def decode(X_encode):\n",
        "  X_reconstruct = idwt(levels=3, wdom_data=X_encode)\n",
        "  return X_reconstruct\n",
        "\n",
        "def k_reconstruct(k,X_dwt):\n",
        "  X_sorted = np.sort(np.absolute(np.array(X_dwt)).flatten(),kind='quicksort')[::-1]\n",
        "  k_threshold = X_sorted[k-1]\n",
        "  X_dwt[np.absolute(X_dwt)<k_threshold] = 0\n",
        "  X_reconstruct = idwt(levels=3, wdom_data=X_dwt)\n",
        "  return X_reconstruct\n",
        "\n",
        "def evaluate_k(k,X):\n",
        "  X_encode = k_encode(k,X)\n",
        "  X_reconstruct = decode(X_encode)\n",
        "  error = frob_error(X_reconstruct,X)\n",
        "  return error\n",
        "\n",
        "def resize_scan(X):\n",
        "    # Raw image is a rectangle, crop to be square\n",
        "    y_min = int((768-396)/2)\n",
        "    y_max = int((768-396)/2+396)\n",
        "    X_square = X[y_min:y_max,:]\n",
        "    # Crop further to zone of interest to get a 256x256 image\n",
        "    X_square_256 = X_square[50:50+256,70:70+256]\n",
        "    return X_square_256\n",
        "\n",
        "def to_jpeg(X,X_true):\n",
        "    matplotlib.image.imsave('../images/X.jpg', X)\n",
        "    image = Image.open('../images/X.jpg').convert('L')\n",
        "    # convert image to numpy array\n",
        "    X_jpeg = np.asarray(image.getdata()).reshape(image.size)\n",
        "    X_jpeg = X_jpeg*np.max(X_true)/255\n",
        "    return X_jpeg\n",
        "\n",
        "def combine_all_coils(volume_kspace,slice_idx):\n",
        "    slice_kspace = volume_kspace[slice_idx]\n",
        "    slice_kspace2 = T.to_tensor(slice_kspace)      # Convert from numpy array to pytorch tensor\n",
        "    slice_image = fastmri.ifft2c(slice_kspace2)           # Apply Inverse Fourier Transform to get the complex image\n",
        "    slice_image_abs = fastmri.complex_abs(slice_image)   # Compute absolute value to get a real image\n",
        "    slice_image_rss = fastmri.rss(slice_image_abs, dim=0)\n",
        "    return slice_image_rss.numpy()"
      ],
      "metadata": {
        "id": "kWF4lrhnKhIr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rjQN_U92KH06"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataloader\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import display\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "\n",
        "import h5py\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "#from utils.common_fcns import *\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UlQ7wpWmKH08"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQX5M94vKH09"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "dgi5lXdHKH0-",
        "outputId": "2fdd25f0-8d4e-4e93-92e5-47971fcc435b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                         slice_name  \\\n",
              "0     file_brain_AXT2_200_6002639_0   \n",
              "1     file_brain_AXT2_200_6002639_1   \n",
              "2     file_brain_AXT2_200_6002639_2   \n",
              "3     file_brain_AXT2_200_6002639_3   \n",
              "4     file_brain_AXT2_200_6002639_4   \n",
              "..                              ...   \n",
              "409  file_brain_AXT2_200_2000368_11   \n",
              "410  file_brain_AXT2_200_2000368_12   \n",
              "411  file_brain_AXT2_200_2000368_13   \n",
              "412  file_brain_AXT2_200_2000368_14   \n",
              "413  file_brain_AXT2_200_2000368_15   \n",
              "\n",
              "                                                 slice  \n",
              "0    [0.00036823840000000004, 0.0001705081, 0.00011...  \n",
              "1    [0.0004485588, 0.00016957770000000001, 0.00011...  \n",
              "2    [0.0003976501, 0.0001861478, 8.638480000000001...  \n",
              "3    [0.00030687520000000003, 0.0001190122, 6.93803...  \n",
              "4    [0.00015227520000000002, 0.0001200416, 0.00010...  \n",
              "..                                                 ...  \n",
              "409  [5.37441e-05, 5.32068e-05, 5.62014e-05, 5.7672...  \n",
              "410  [5.14185e-05, 5.08681e-05, 5.10866e-05, 5.3309...  \n",
              "411  [5.53217e-05, 5.2996000000000006e-05, 5.15385e...  \n",
              "412  [5.1762700000000005e-05, 5.11068e-05, 5.05937e...  \n",
              "413  [5.48294e-05, 5.3422100000000005e-05, 5.427170...  \n",
              "\n",
              "[414 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8424aa74-7e58-448f-870a-95a7e2f92afe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slice_name</th>\n",
              "      <th>slice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>file_brain_AXT2_200_6002639_0</td>\n",
              "      <td>[0.00036823840000000004, 0.0001705081, 0.00011...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>file_brain_AXT2_200_6002639_1</td>\n",
              "      <td>[0.0004485588, 0.00016957770000000001, 0.00011...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>file_brain_AXT2_200_6002639_2</td>\n",
              "      <td>[0.0003976501, 0.0001861478, 8.638480000000001...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>file_brain_AXT2_200_6002639_3</td>\n",
              "      <td>[0.00030687520000000003, 0.0001190122, 6.93803...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>file_brain_AXT2_200_6002639_4</td>\n",
              "      <td>[0.00015227520000000002, 0.0001200416, 0.00010...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>file_brain_AXT2_200_2000368_11</td>\n",
              "      <td>[5.37441e-05, 5.32068e-05, 5.62014e-05, 5.7672...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>file_brain_AXT2_200_2000368_12</td>\n",
              "      <td>[5.14185e-05, 5.08681e-05, 5.10866e-05, 5.3309...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>file_brain_AXT2_200_2000368_13</td>\n",
              "      <td>[5.53217e-05, 5.2996000000000006e-05, 5.15385e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>file_brain_AXT2_200_2000368_14</td>\n",
              "      <td>[5.1762700000000005e-05, 5.11068e-05, 5.05937e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>file_brain_AXT2_200_2000368_15</td>\n",
              "      <td>[5.48294e-05, 5.3422100000000005e-05, 5.427170...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>414 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8424aa74-7e58-448f-870a-95a7e2f92afe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8424aa74-7e58-448f-870a-95a7e2f92afe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8424aa74-7e58-448f-870a-95a7e2f92afe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# upload train_set.json and test_set.json under /data/ in colab 'Files' tab\n",
        "# these can be generated with scripts/prepare_training_data.py in the repo\n",
        "\n",
        "train_df = pd.read_json('/data/train_set.json')\n",
        "test_df = pd.read_json('/data/test_set.json')\n",
        "display(train_df)\n",
        "\n",
        "class Data(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super(Data, self).__init__()\n",
        "        self.x = torch.tensor(x)\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.y.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "# Send all but first brain to training, reserve first brain for test\n",
        "train_slices = train_df['slice']\n",
        "data = Data(train_slices,train_slices)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slice = data[0]\n",
        "#slice = np.array(slice[0])\n",
        "print(slice[0].shape) #Verify the shape is 256x256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE3YCv-pPIiK",
        "outputId": "33d86d99-f8b1-4971-a55d-323239a41013"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([65536])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRK1XiGwKH0_"
      },
      "source": [
        "### Define the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N2prjtbxKH0_"
      },
      "outputs": [],
      "source": [
        "# Inherit from torch.nn, define basic k-latent AutoEncoder\n",
        "k_latent = 3000\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=8, stride=2, padding=1), \n",
        "            nn.ReLU(),\n",
        "            #nn.MaxPool2d(2, stride=None), \n",
        "            nn.Conv2d(32, 16, kernel_size=5, stride=2, padding=2), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, kernel_size=1, stride=2, padding=1), \n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        #self.latent = nn.Sequential(\n",
        "        #    nn.Linear(5776, k_latent),\n",
        "        #    nn.Linear(k_latent,5776)\n",
        "        #)\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.ConvTranspose2d(16, 16, kernel_size=2, stride=2, padding=1,),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 32, kernel_size=5, stride=2, padding=2,),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=8, stride=2, padding=2,),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        #x = x.view(x.size(0), -1) # Flatten for latent layer\n",
        "        #x = self.latent(x)\n",
        "        x = self.decode(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv6GgApXKH1A"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hUgOHXAcKH1A"
      },
      "outputs": [],
      "source": [
        "# Training setup\n",
        "n_epochs = 5\n",
        "batch_size = 414 #Full batch\n",
        "batch_loss = np.zeros(n_epochs)\n",
        "\n",
        "model = AutoEncoder()\n",
        "#model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1000)\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7BaaG8zKH1A",
        "outputId": "ec15a096-962e-42c8-e5a6-1db0e883656a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "AutoEncoder                              --                        --\n",
              "├─Sequential: 1-1                        [1, 16, 33, 33]           --\n",
              "│    └─Conv2d: 2-1                       [1, 32, 126, 126]         2,080\n",
              "│    └─ReLU: 2-2                         [1, 32, 126, 126]         --\n",
              "│    └─Conv2d: 2-3                       [1, 16, 63, 63]           12,816\n",
              "│    └─ReLU: 2-4                         [1, 16, 63, 63]           --\n",
              "│    └─Conv2d: 2-5                       [1, 16, 33, 33]           272\n",
              "│    └─ReLU: 2-6                         [1, 16, 33, 33]           --\n",
              "├─Sequential: 1-2                        [1, 1, 256, 256]          --\n",
              "│    └─ConvTranspose2d: 2-7              [1, 16, 64, 64]           1,040\n",
              "│    └─ReLU: 2-8                         [1, 16, 64, 64]           --\n",
              "│    └─ConvTranspose2d: 2-9              [1, 32, 127, 127]         12,832\n",
              "│    └─ReLU: 2-10                        [1, 32, 127, 127]         --\n",
              "│    └─ConvTranspose2d: 2-11             [1, 1, 256, 256]          2,049\n",
              "│    └─Sigmoid: 2-12                     [1, 1, 256, 256]          --\n",
              "==========================================================================================\n",
              "Total params: 31,089\n",
              "Trainable params: 31,089\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 429.70\n",
              "==========================================================================================\n",
              "Input size (MB): 0.26\n",
              "Forward/backward pass size (MB): 9.89\n",
              "Params size (MB): 0.12\n",
              "Estimated Total Size (MB): 10.28\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "summary(model,input_size=(1,1,256,256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIEek0VkKH1B",
        "outputId": "0c8fbc8a-6217-4b4f-d807-bce11f216fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "Completed epoch: 0 , Loss: 0.26192376017570496\n",
            "A\n",
            "Completed epoch: 1 , Loss: 2.1623542068027746e-08\n",
            "A\n",
            "Completed epoch: 2 , Loss: 2.1623542068027746e-08\n",
            "A\n",
            "Completed epoch: 3 , Loss: 2.1623542068027746e-08\n",
            "A\n",
            "Completed epoch: 4 , Loss: 2.1623542068027746e-08\n",
            "A\n",
            "Completed epoch: 5 , Loss: 2.1623540291670906e-08\n",
            "A\n",
            "Completed epoch: 6 , Loss: 2.1623542068027746e-08\n",
            "A\n",
            "Completed epoch: 7 , Loss: 2.1623542068027746e-08\n",
            "A\n",
            "Completed epoch: 8 , Loss: 2.1623542068027746e-08\n",
            "A\n",
            "Completed epoch: 9 , Loss: 2.1623542068027746e-08\n"
          ]
        }
      ],
      "source": [
        "# Do training\n",
        "for epoch in range(n_epochs):\n",
        "    # Pick batches for this epoch\n",
        "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "    for batch_idx, (X, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()   # zero the gradient buffers\n",
        "        X = torch.reshape(X,(batch_size,1,256,256))\n",
        "        Y = model(X)\n",
        "        loss = criterion(X,Y)\n",
        "        batch_loss[epoch]+=loss.item() # save off the loss\n",
        "        loss.backward()\n",
        "        optimizer.step()    # update\n",
        "    print(\"Completed epoch: \"+str(epoch)+\" , Loss: \"+str(batch_loss[epoch]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFsO2wf4KH1B"
      },
      "source": [
        "### Test It"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wzwHTS8rKH1B"
      },
      "outputs": [],
      "source": [
        "# Load a test scan in\n",
        "test_slices = test_df['slice']\n",
        "X_input = torch.tensor(test_slices[0])\n",
        "X_input = torch.reshape(X_input,(1,1,256,256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "M23PI4EEKH1C",
        "outputId": "40398847-8576-48ae-d831-d2a32b6b7145"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 255.5, 255.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACy0lEQVR4nO3YMQoDMQwAwdOR/39Z+YBJF7zFTCk3ahaBZ3cfoOe9vQBwJk6IEidEiROixAlRn1+PM+MrF/5sd+c0dzkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghanb39g7AgcsJUeKEKHFClDghSpwQJU6I+gI34gvJPjrjHQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Put it through our autoencoder and see what we get!\n",
        "X_output = model(X_input)\n",
        "X_output = X_output.detach().numpy().reshape(256,256)\n",
        "X_reconstruct = decode(X_output)\n",
        "plt.imshow(np.abs(X_output), cmap='gray')\n",
        "plt.axis('off')\n",
        "#plt.savefig('../images/cnn_wavelet_reconstruction.jpg', dpi=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "-vRLQQ-rKH1C",
        "outputId": "2da74b08-a080-4978-aae5-5e4458c3328a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 255.5, 255.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACy0lEQVR4nO3YMQoDMQwAwdOR/39Z+YBJF7zFTCk3ahaBZ3cfoOe9vQBwJk6IEidEiROixAlRn1+PM+MrF/5sd+c0dzkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghanb39g7AgcsJUeKEKHFClDghSpwQJU6I+gI34gvJPjrjHQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(np.abs(X_reconstruct), cmap='gray')\n",
        "plt.axis('off')\n",
        "#plt.savefig('../images/cnn_reconstruction.jpg', dpi=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3kbwy5DKH1C"
      },
      "source": [
        "Yikes. Looks like we recovered the wavelets somewhat, but the true image reconstruction is a disaster. A CNN will do better than our shallow NN."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "sparse_auto_encoder_cnn.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}