{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataloader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_name</th>\n",
       "      <th>slice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file_brain_AXT2_200_2000535_0</td>\n",
       "      <td>[0.0029112822, 0.0022742343, 0.0024876709, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file_brain_AXT2_200_2000535_1</td>\n",
       "      <td>[0.00254231, 0.0018121544, 0.0022545222, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file_brain_AXT2_200_2000535_2</td>\n",
       "      <td>[0.0021948142, 0.001427773, 0.0019365047, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file_brain_AXT2_200_2000535_3</td>\n",
       "      <td>[0.0019062636, 0.0010817663, 0.0016660133, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file_brain_AXT2_200_2000535_4</td>\n",
       "      <td>[0.001538842, 0.00081564067, 0.0013866577, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>file_brain_AXT2_200_2000368_11</td>\n",
       "      <td>[5.3744112e-05, 5.3206768e-05, 5.620142e-05, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>file_brain_AXT2_200_2000368_12</td>\n",
       "      <td>[5.141851e-05, 5.086812e-05, 5.1086612e-05, 5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>file_brain_AXT2_200_2000368_13</td>\n",
       "      <td>[5.5321667e-05, 5.2995976e-05, 5.153848e-05, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>file_brain_AXT2_200_2000368_14</td>\n",
       "      <td>[5.176266e-05, 5.110678e-05, 5.0593735e-05, 5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>file_brain_AXT2_200_2000368_15</td>\n",
       "      <td>[5.482942e-05, 5.3422093e-05, 5.427175e-05, 5....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>430 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         slice_name  \\\n",
       "0     file_brain_AXT2_200_2000535_0   \n",
       "1     file_brain_AXT2_200_2000535_1   \n",
       "2     file_brain_AXT2_200_2000535_2   \n",
       "3     file_brain_AXT2_200_2000535_3   \n",
       "4     file_brain_AXT2_200_2000535_4   \n",
       "..                              ...   \n",
       "425  file_brain_AXT2_200_2000368_11   \n",
       "426  file_brain_AXT2_200_2000368_12   \n",
       "427  file_brain_AXT2_200_2000368_13   \n",
       "428  file_brain_AXT2_200_2000368_14   \n",
       "429  file_brain_AXT2_200_2000368_15   \n",
       "\n",
       "                                                 slice  \n",
       "0    [0.0029112822, 0.0022742343, 0.0024876709, 0.0...  \n",
       "1    [0.00254231, 0.0018121544, 0.0022545222, 0.002...  \n",
       "2    [0.0021948142, 0.001427773, 0.0019365047, 0.00...  \n",
       "3    [0.0019062636, 0.0010817663, 0.0016660133, 0.0...  \n",
       "4    [0.001538842, 0.00081564067, 0.0013866577, 0.0...  \n",
       "..                                                 ...  \n",
       "425  [5.3744112e-05, 5.3206768e-05, 5.620142e-05, 5...  \n",
       "426  [5.141851e-05, 5.086812e-05, 5.1086612e-05, 5....  \n",
       "427  [5.5321667e-05, 5.2995976e-05, 5.153848e-05, 5...  \n",
       "428  [5.176266e-05, 5.110678e-05, 5.0593735e-05, 5....  \n",
       "429  [5.482942e-05, 5.3422093e-05, 5.427175e-05, 5....  \n",
       "\n",
       "[430 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment the below line if you haven't created the dataset file yet\n",
    "#!python prepare_training_data.py \n",
    "\n",
    "# Load in the data file\n",
    "df = pd.read_pickle('../data/training_set.pkl')\n",
    "display(df)\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(Data, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "train_slices = df['slice'].to_numpy()\n",
    "data = Data(train_slices,train_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherit from torch.nn, define basic k-latent AutoEncoder\n",
    "k_latent = 2500\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.k_latent_encode = nn.Linear(256*256, k_latent)\n",
    "        self.k_latent_decode = nn.Linear(k_latent, 256*256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.k_latent_encode(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.k_latent_decode(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000007vscode-remote?line=3'>4</a>\u001b[0m batch_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(n_epochs)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000007vscode-remote?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoEncoder()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000007vscode-remote?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000007vscode-remote?line=7'>8</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000007vscode-remote?line=8'>9</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:907\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=906'>907</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:578\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=575'>576</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=576'>577</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=577'>578</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=581'>582</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=582'>583</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=587'>588</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:601\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=596'>597</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=597'>598</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=598'>599</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=599'>600</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=600'>601</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=601'>602</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=602'>603</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=901'>902</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=902'>903</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=903'>904</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=904'>905</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=211'>212</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=212'>213</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=213'>214</a>\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=214'>215</a>\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=215'>216</a>\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=216'>217</a>\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=217'>218</a>\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=218'>219</a>\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=219'>220</a>\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "n_epochs = 100\n",
    "minibatch_size = 16\n",
    "batch_loss = np.zeros(n_epochs)\n",
    "\n",
    "model = AutoEncoder()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch: 0 , Loss: [13.49960357  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Completed epoch: 1 , Loss: [13.49960357 13.49957031  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Completed epoch: 2 , Loss: [13.49960357 13.49957031 13.49953763  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000006vscode-remote?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(X,Y)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000006vscode-remote?line=9'>10</a>\u001b[0m     batch_loss[epoch]\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem() \u001b[39m# save off the loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000006vscode-remote?line=10'>11</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000006vscode-remote?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()    \u001b[39m# update\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alex/git/MRI_Compression/scripts/sparse_auto_encoder.ipynb#ch0000006vscode-remote?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCompleted epoch: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(epoch)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m , Loss: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(batch_loss))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/alex/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Do training\n",
    "for epoch in range(n_epochs):\n",
    "    # Pick batches for this epoch\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=minibatch_size, shuffle=True)\n",
    "    for batch_idx, (X, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        Y = model(X)\n",
    "        loss = criterion(X,Y)\n",
    "        batch_loss[epoch]+=loss.item() # save off the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()    # update\n",
    "    print(\"Completed epoch: \"+str(epoch)+\" , Loss: \"+str(batch_loss[epoch]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
